# ğŸ•·ï¸ Go Web Crawler

A fast, concurrent web crawler written in Go.  
It crawls a given website, collects all **internal links**, and generates a **report** showing how many times each page is linked internally.

---

## âœ¨ Features

- **âš¡ Concurrent crawling** â€“ multiple pages fetched in parallel for speed.
- **ğŸ“Œ Configurable max concurrency** â€“ limit simultaneous HTTP requests.
- **ğŸ“„ Configurable max pages** â€“ stop crawling after a set number of pages.
- **ğŸ”’ Domain restriction** â€“ only crawls links within the same domain.
- **ğŸ“Š Sorted report output** â€“ most linked pages appear first.
- **ğŸ›¡ï¸ Threadâ€‘safe** â€“ safe access to shared state using `sync.Mutex`.

---

## ğŸ“¦ Installation

Clone the repository:
```bash
git clone https://github.com/somu-codes/crawler.git
cd crawler
```

Build the binary:
```bash
go build -o crawler
```

Run the binary:

```bash
go run . <url> [maxConcurrency] [maxPages]
```

